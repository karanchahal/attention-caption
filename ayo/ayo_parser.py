# -*- coding: utf-8 -*-
"""pytorch_feedforward_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c

# Check Python version
"""

import sys
sys.version

"""# Install PyTorch"""

!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl

!pip3 install torchvision



from __future__ import unicode_literals, print_function, division
from nltk.tokenize.casual import TweetTokenizer
import pickle
import torch
from io import open
import unicodedata
import string
import re
import random
import torch.nn as nn
from torch.autograd import Variable
from torch import optim
import torch.nn.functional as F
import operator

use_cuda = torch.cuda.is_available()



import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np


MAX_LENGTH = 20




class Vocab:
    
    def __init__(self):
        self.id2word = []
        self.word2id = {}
        self.wordfreq = {}
        self.unknowns = 0
        self.id2word.append('<SOS>')
        self.id2word.append('<EOS>')
        self.id2word.append('<UNK>')
        self.word2id['<SOS>'] = 0
        self.word2id['<EOS>'] = 1
        self.word2id['<UNK>'] = 2

        self.id = 3
        self.examples = 0

    def add(self,word):
        if word not in self.word2id:
            self.word2id[word] = self.id
            self.wordfreq[word] = 1
            self.id2word.append(word)
            self.id +=1
        else:
            self.wordfreq[word] += 1
    
    def word(self,id):
        return self.id2word[id]
    
    def idx(self,word):
        if word in self.word2id:
            return self.word2id[word]
        else:
            self.unknowns +=1
            return self.word2id['<UNK>']
    
    def length(self):
        return self.id






def trim(s):
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?:;)(*',/üåªüêΩ‚ù§9üñïü¶Å8üò∂üò≥üòî#<3]+", r" ", s)
    return s


def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)


tokenizer = TweetTokenizer()

ayo = 1
karan = 0
grammer = ''
karan_answers = []
ayo_questions = []

# Lowercase, trim, and remove non-letter characters

def normalizeString(s):
    s = s.lower().strip()
    return s
  


def get_variable(indexes):
    result = Variable(torch.cuda.LongTensor(indexes).view(-1, 1))
    return result

def get_sentence(indexes):
    sen = ''
    for i in indexes:
        sen += ' ' + vocab.word(i)
    return sen

def get_indices(sentence):
    i = tokenizer.tokenize(i)
    indices = []
    for j in i:
        indices.append(vocab.idx(j))
    
    return indices

class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        result = Variable(torch.zeros(1, 1, self.hidden_size))
        if use_cuda:
            return result.cuda()
        else:
            return result

######################################################################
# The Decoder
# -----------
#
# The decoder is another RNN that takes the encoder output vector(s) and
# outputs a sequence of words to create the translation.
#


######################################################################
# Simple Decoder
# ^^^^^^^^^^^^^^
#
# In the simplest seq2seq decoder we use only last output of the encoder.
# This last output is sometimes called the *context vector* as it encodes
# context from the entire sequence. This context vector is used as the
# initial hidden state of the decoder.
#
# At every step of decoding, the decoder is given an input token and
# hidden state. The initial input token is the start-of-string ``<SOS>``
# token, and the first hidden state is the context vector (the encoder's
# last hidden state).
#
# .. figure:: /_static/img/seq-seq-images/decoder-network.png
#    :alt:
#
#

class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax()

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        result = Variable(torch.zeros(1, 1, self.hidden_size))
        if use_cuda:
            return result.cuda()
        else:
            return result

######################################################################
# I encourage you to train and observe the results of this model, but to
# save space we'll be going straight for the gold and introducing the
# Attention Mechanism.
#


######################################################################
# Attention Decoder
# ^^^^^^^^^^^^^^^^^
#
# If only the context vector is passed betweeen the encoder and decoder,
# that single vector carries the burden of encoding the entire sentence.
#
# Attention allows the decoder network to "focus" on a different part of
# the encoder's outputs for every step of the decoder's own outputs. First
# we calculate a set of *attention weights*. These will be multiplied by
# the encoder output vectors to create a weighted combination. The result
# (called ``attn_applied`` in the code) should contain information about
# that specific part of the input sequence, and thus help the decoder
# choose the right output words.
#
# .. figure:: https://i.imgur.com/1152PYf.png
#    :alt:
#
# Calculating the attention weights is done with another feed-forward
# layer ``attn``, using the decoder's input and hidden state as inputs.
# Because there are sentences of all sizes in the training data, to
# actually create and train this layer we have to choose a maximum
# sentence length (input length, for encoder outputs) that it can apply
# to. Sentences of the maximum length will use all the attention weights,
# while shorter sentences will only use the first few.
#
# .. figure:: /_static/img/seq-seq-images/attention-decoder-network.png
#    :alt:
#
#

class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)))
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, hidden = self.gru(output, hidden)

        output = F.log_softmax(self.out(output[0]))
        return output, hidden, attn_weights

    def initHidden(self):
        result = Variable(torch.zeros(1, 1, self.hidden_size))
        if use_cuda:
            return result.cuda()
        else:
            return result




teacher_forcing_ratio = 0.5


def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_variable.size()[0]
    target_length = target_variable.size()[0]

    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))
    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(
            input_variable[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0][0]

    decoder_input = Variable(torch.LongTensor([[vocab.idx('<SOS>')]]))
    decoder_input = decoder_input.cuda() if use_cuda else decoder_input

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_variable[di])
            decoder_input = target_variable[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.data.topk(1)
            ni = topi[0][0]

            decoder_input = Variable(torch.LongTensor([[ni]]))
            decoder_input = decoder_input.cuda() if use_cuda else decoder_input

            loss += criterion(decoder_output, target_variable[di])
            if ni == vocab.idx('<EOS>'):
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.data[0] / target_length


######################################################################
# This is a helper function to print time elapsed and estimated time
# remaining given the current time and progress %.
#

import time
import math


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))


def get_data_element(training_example):
    input_variable = get_variable(training_example[0])
    target_variable = get_variable(training_example[1])
    return input_variable, target_variable


def zipper(x,y):
    i = len(x)
    print(i)
    pairs = []
    for j in range(i):
        pair = []

        pair.append(x[j])
        pair.append(y[j])

        pairs.append(pair)
    
    return pairs

def trainIters(encoder, decoder, x, y, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    start = time.time()
    plot_losses = []

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    pairs = zipper(x,y)
    training_examples = pairs
    criterion = nn.NLLLoss()
    num_examples = len(training_examples)
    for epoch in range(1, n_iters + 1):
        print_loss_total = 0  # Reset every print_every
        plot_loss_total = 0  # Reset every plot_every

        for i in range(len(training_examples)):

            training_example = training_examples[i]
            input_variable, target_variable = get_data_element(training_example)

            loss = train(input_variable, target_variable, encoder,
                        decoder, encoder_optimizer, decoder_optimizer, criterion)
            print_loss_total += loss
            plot_loss_total += loss

            if i % print_every == 0:
                print_loss_avg = print_loss_total / print_every
                print_loss_total = 0
                print('%s Epoch %d  (%d %d%%) %.4f' % (timeSince(start, i+1 / num_examples),epoch,
                                            i, i / num_examples * 100, print_loss_avg))

            if i % plot_every == 0:
                plot_loss_avg = plot_loss_total / plot_every
                plot_losses.append(plot_loss_avg)
                plot_loss_total = 0

    showPlot(plot_losses)


def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    
    EOS_token = vocab.idx('<EOS>')
    SOS_token = vocab.idx('<SOS>')
    input_variable = get_variable(sentence)
    input_length = input_variable.size()[0]
    encoder_hidden = encoder.initHidden()

    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))
    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(input_variable[ei],
                                                 encoder_hidden)
        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]

    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS
    decoder_input = decoder_input.cuda() if use_cuda else decoder_input

    decoder_hidden = encoder_hidden

    decoded_words = []
    decoder_attentions = torch.zeros(max_length, max_length)

    for di in range(max_length):
        decoder_output, decoder_hidden, decoder_attention = decoder(
            decoder_input, decoder_hidden, encoder_outputs)
        decoder_attentions[di] = decoder_attention.data
        topv, topi = decoder_output.data.topk(1)
        ni = topi[0][0]
        if ni == EOS_token:
            decoded_words.append('<EOS>')
            break
        else:
            decoded_words.append(vocab.word(ni))

        decoder_input = Variable(torch.LongTensor([[ni]]))
        decoder_input = decoder_input.cuda() if use_cuda else decoder_input

    return decoded_words, decoder_attentions[:di + 1]


######################################################################
# We can evaluate random sentences from the training set and print out the
# input, target, and output to make some subjective quality judgements:
#

def evaluateRandomly(encoder, x, y, decoder, n=3):
    pairs = zipper(x,y)
    for i in range(n):
        
        pair = random.choice(pairs)

        print('>', get_sentence(pair[0]))
        print('=', get_sentence(pair[1]))
        output_words, attentions = evaluate(encoder, decoder, pair[0])
        print(output_words)
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')


def get_length(a,length):
  for i in a:
    length = max(length,len(i))
  return length  
  


 # Read the file and split into lines
lines = open('./chatlog/ayo.txt', encoding='utf-8').\
    read().strip().split('\n')


for line in lines:

    try:
        k = trim(normalizeString(line.split('Karanbir Singh Chahal:')[1]))
        grammer += ' . ' + k
        if karan == 0:
            tmp = k
            karan_answers.append(tmp)
            karan = 1
            ayo = 0
        else:
            tmp = karan_answers[-1]
            tmp = trim(tmp + ' . ' + k)
            karan_answers[-1] = tmp

    except Exception as e:
        pass

    try:   
        a = trim(normalizeString(line.split('Ayotakshee:')[1]))
        grammer += ' . ' + a
        if ayo == 0:
            tmp = a
            ayo_questions.append(tmp)
            ayo = 1
            karan = 0
        else:
            tmp = ayo_questions[-1]
            tmp = trim(tmp + ' . ' + a)
            ayo_questions[-1] = tmp

    except Exception as e:
        pass


grammer = trim(grammer.lower().strip())
print(grammer[:1100])
tokens = tokenizer.tokenize(grammer)
vocab = Vocab()
for t in tokens:
    vocab.add(t)

main_k_answers = []

for i in karan_answers:
    i = tokenizer.tokenize(i)
    k_tokens = []
    for j in i:
        k_tokens.append(vocab.idx(j))

    k_tokens.append(vocab.idx('<EOS>'))
    main_k_answers.append(k_tokens)


main_a_questions = []

for i in ayo_questions:
    i = tokenizer.tokenize(i)
    a_tokens = []
    for j in i:
        a_tokens.append(vocab.idx(j))
    a_tokens.append(vocab.idx('<EOS>'))
    main_a_questions.append(a_tokens)

    
main_k = []
main_a = []

for i in range(len(main_k_answers)):
    if len(main_k_answers[i]) < MAX_LENGTH and len(main_a_questions[i]) < MAX_LENGTH:
        main_k.append(main_k_answers[i])
        main_a.append(main_a_questions[i])

main_k_answers = main_k
main_a_questions = main_a

        
vocab_length = vocab.length()
word_freq = sorted(vocab.wordfreq.items(), key=operator.itemgetter(1))


hidden_size = 256
encoder1 = EncoderRNN(vocab_length, hidden_size)
attn_decoder1 = AttnDecoderRNN(hidden_size,vocab_length , dropout_p=0.1)
print(MAX_LENGTH, "is maximum length of sentence")
print('Vocab size is ', vocab_length)
print('Number of examples in one epoch are', len(main_a_questions))
print('NUmber of Unknown words are ', vocab.unknowns)

print(use_cuda)

if use_cuda:
    encoder1 = encoder1.cuda()
    attn_decoder1 = attn_decoder1.cuda()

from google.colab import files

trainIters(encoder1, attn_decoder1,main_a_questions,main_k_answers, 10, print_every=100,learning_rate=0.01)
evaluateRandomly(encoder1,main_a_questions,main_k_answers, attn_decoder1)

torch.save(encoder1.state_dict(), 'encoder.tar')
torch.save(attn_decoder1.state_dict(),'decoder.tar')

files.download('./encoder.tar')
files.download('./decoder.tar')


trainIters(encoder1, attn_decoder1,main_a_questions,main_k_answers, 20, print_every=100,learning_rate=0.001)
evaluateRandomly(encoder1,main_a_questions,main_k_answers, attn_decoder1)

torch.save(encoder1.state_dict(), 'encoder.tar')
torch.save(attn_decoder1.state_dict(),'decoder.tar')

files.download('./encoder.tar')
files.download('./decoder.tar')


trainIters(encoder1, attn_decoder1,main_a_questions,main_k_answers, 40, print_every=100,learning_rate=0.0001)
evaluateRandomly(encoder1,main_a_questions,main_k_answers, attn_decoder1)

torch.save(encoder1.state_dict(), './encoder.tar')
torch.save(attn_decoder1.state_dict(),'./decoder.tar')

files.download('./encoder.tar')
files.download('./decoder.tar')

trainIters(encoder1, attn_decoder1,main_a_questions,main_k_answers, 20, print_every=100,learning_rate=0.001)
evaluateRandomly(encoder1,main_a_questions,main_k_answers, attn_decoder1)

torch.save(encoder1.state_dict(), 'encoder.tar')
torch.save(attn_decoder1.state_dict(),'decoder.tar')